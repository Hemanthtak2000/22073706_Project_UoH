{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hemanthtak2000/22073706_Project_UoH/blob/main/Code_research.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages that needed to execute the code\n",
        "!pip install -q transformers datasets evaluate"
      ],
      "metadata": {
        "id": "AyU4gLYkCh27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the libraries needed to execute the code\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report,ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "bAd-AEZBCkIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize lemmatizer\n",
        "Email_Lem = WordNetLemmatizer()\n",
        "\n",
        "# Load English stopwords\n",
        "Email_StopW = set(stopwords.words('english'))\n",
        "\n",
        "def Clean_the_textData(text):\n",
        "    \"\"\"\n",
        "    Clean and preprocess a given text string.\n",
        "\n",
        "    Steps performed:\n",
        "    1. Convert text to lowercase.\n",
        "    2. Remove all punctuation and numeric characters.\n",
        "    3. Tokenize the cleaned text into words.\n",
        "    4. Remove English stopwords (e.g., 'the', 'is', 'and').\n",
        "    5. Lemmatize each word to its base form.\n",
        "    6. Rejoin the processed words into a single string.\n",
        "\n",
        "    Parameters:\n",
        "    text : Raw input text.\n",
        "\n",
        "    Returns: Cleaned and preprocessed text string.\n",
        "    \"\"\"\n",
        "    tolowercase = text.lower()\n",
        "    toremoveAl = re.sub(r'[^a-z\\s]', '', tolowercase)\n",
        "    Toremovetokens = toremoveAl.split()\n",
        "    toLemmatize = [Email_Lem.lemmatize(word) for word in Toremovetokens if word not in Email_StopW]\n",
        "    return ' '.join(toLemmatize)\n"
      ],
      "metadata": {
        "id": "Mb4qCDwdGbLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset from CSV file\n",
        "Email_Spam_Data = pd.read_csv(\"https://raw.githubusercontent.com/Hemanthtak2000/ResearchAssignment/refs/heads/main/Job_Dataset.csv\")"
      ],
      "metadata": {
        "id": "fYRnmMDmCz1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display first 5 rows of the dataset\n",
        "print(Email_Spam_Data.head())"
      ],
      "metadata": {
        "id": "Z-Ctv0UVDVht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show column data types and non-null counts\n",
        "Email_Spam_Data.info()"
      ],
      "metadata": {
        "id": "wdWO5HnrGEBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename columns: assume the current columns are something like 'fraudulent' and 'description'\n",
        "Email_Spam_Data = Email_Spam_Data.rename(columns={\n",
        "    'fraudulent': 'label',\n",
        "    'Job_Desc': 'text'\n",
        "})\n",
        "print(Email_Spam_Data.columns)"
      ],
      "metadata": {
        "id": "AOgvbU2GWSHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show column data types and non-null counts\n",
        "Email_Spam_Data.info()"
      ],
      "metadata": {
        "id": "tkQpqc_OWVd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show stats for numerical columns\n",
        "Email_Spam_Data.describe()"
      ],
      "metadata": {
        "id": "fANwFrXzDh-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print dataset shape to know the how many columns and rows are these.\n",
        "Email_df_rows, Email_df_columns = Email_Spam_Data.shape\n",
        "print(f\"The shape of the original dataset is {Email_df_rows} reviews with {Email_df_columns} columns.\")"
      ],
      "metadata": {
        "id": "mCgkd6IfF0rN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show null value count per column\n",
        "Email_Spam_Data.isna().sum()"
      ],
      "metadata": {
        "id": "wFxKmp34F6dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new column with the length of each comment\n",
        "Email_Spam_Data['Email_Len'] = Email_Spam_Data['text'].apply(len)"
      ],
      "metadata": {
        "id": "XW0acMIfHgJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to text column\n",
        "Email_Spam_Data['Email_Cleaned'] = Email_Spam_Data['text'].apply(Clean_the_textData)\n",
        "\n",
        "# Show the difference between original and cleaned text columns\n",
        "Email_Spam_Data[['text', 'Email_Cleaned']]\n"
      ],
      "metadata": {
        "id": "JNy4jpMtGCzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicate rows in the dataset\n",
        "print(f\"Number of duplicate rows in the Email Dataset : {Email_Spam_Data.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "5agf5GFSHrux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show column names after preprocessing\n",
        "Email_Spam_Data.columns"
      ],
      "metadata": {
        "id": "BEJVSUDkH1xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display first 5 rows of the updated dataset\n",
        "Email_Spam_Data.head()"
      ],
      "metadata": {
        "id": "U79J1C_nH-Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check and display class label distribution\n",
        "Cnt_Tgt = Email_Spam_Data['label'].value_counts()\n",
        "print(\"Label Distribution:\\n\", Cnt_Tgt)"
      ],
      "metadata": {
        "id": "NHTulqW8IKe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the count of class labels\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='label', data=Email_Spam_Data)\n",
        "plt.title(\"Email Classification Label Count\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e8GXQAweIbzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot histogram of text lengths\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(Email_Spam_Data['Email_Len'], bins=30, kde=True)\n",
        "plt.title(\"Distribution of Text Lengths of Email Data\")\n",
        "plt.xlabel(\"Number of Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gcMK53BpIqRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate WordCloud for each class label\n",
        "for tgt in sorted(Email_Spam_Data['label'].unique()):\n",
        "    EM_txt = ' '.join(Email_Spam_Data[Email_Spam_Data['label'] == tgt]['Email_Cleaned'])\n",
        "    Gen_Wrdcld = WordCloud(width=800, height=400, background_color='white').generate(EM_txt)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(Gen_Wrdcld)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"WordCloud for Label {tgt}\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "suX6lOntJ4ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify columns for correlation analysis\n",
        "Num_Col_Email = ['label', 'Email_Len']\n",
        "# Compute correlation matrix\n",
        "Cormat_EM = Email_Spam_Data[Num_Col_Email].corr()\n",
        "# Plot heatmap of correlations\n",
        "sns.heatmap(Cormat_EM, annot=True)\n",
        "# Add title to the heatmap\n",
        "plt.title('Correlation Heatmap of Label and Text Length')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ocK7gS3jKUY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "# Separate majority and minority classes\n",
        "majority_class = Email_Spam_Data[Email_Spam_Data['label'] == 0]\n",
        "minority_class = Email_Spam_Data[Email_Spam_Data['label'] == 1]\n",
        "\n",
        "# Upsample minority class to match majority class\n",
        "minority_upsampled = resample(\n",
        "    minority_class,\n",
        "    replace=True,\n",
        "    n_samples=len(majority_class),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Combine majority and upsampled minority class\n",
        "Email_Spam_Data_balanced = pd.concat([majority_class, minority_upsampled])\n",
        "\n",
        "# Shuffle the dataset\n",
        "Email_Spam_Data_balanced = Email_Spam_Data_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "QirAT83gZUA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot label distribution after oversampling\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='label', data=Email_Spam_Data_balanced)\n",
        "plt.title(\"Label Distribution After Oversampling\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Hep1k3EMZkuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into training and testing sets with stratified labels\n",
        "Email_Train_df, Email_Test_df = train_test_split(\n",
        "    Email_Spam_Data, test_size=0.2, random_state=2025, stratify=Email_Spam_Data['label']\n",
        ")"
      ],
      "metadata": {
        "id": "9RBgekqQKtUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert train and test DataFrames to HuggingFace Dataset format\n",
        "Em_Train_DS = Dataset.from_pandas(Email_Train_df)\n",
        "Em_Test_DS = Dataset.from_pandas(Email_Test_df)"
      ],
      "metadata": {
        "id": "Qa4fi2qGLYB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BERT tokenizer\n",
        "Chosen_model_EM = \"bert-base-uncased\"\n",
        "Em_Bert_Token = AutoTokenizer.from_pretrained(Chosen_model_EM)"
      ],
      "metadata": {
        "id": "fWtP6qoqLfgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization function for HuggingFace datasets\n",
        "def fun_Token_EM(data):\n",
        "    \"\"\"\n",
        "    Tokenizes input text using the loaded tokenizer.Applies padding and\n",
        "    truncation to ensure uniform input length for transformer models\n",
        "    like BERT or RoBERTa.\n",
        "\n",
        "    Parameters:\n",
        "    data: A batch of examples with a \"text\" field.\n",
        "\n",
        "    Returns: Tokenized output with input IDs, attention masks, etc.\n",
        "    \"\"\"\n",
        "    return Em_Bert_Token(data[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "\n",
        "# Apply tokenization to train and test datasets\n",
        "Em_Train_DS = Em_Train_DS.map(fun_Token_EM, batched=True)\n",
        "Em_Test_DS = Em_Test_DS.map(fun_Token_EM, batched=True)"
      ],
      "metadata": {
        "id": "rWUv_px2L2vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model for sequence classification with 2 output labels\n",
        "Cust_Model_EM = AutoModelForSequenceClassification.from_pretrained(Chosen_model_EM, num_labels=2)"
      ],
      "metadata": {
        "id": "DCh1t-LWMcr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set training parameters for the model\n",
        "Em_Btch_Size = 64\n",
        "EM_Log_Steps = len(Em_Train_DS) // Em_Btch_Size\n",
        "# Get the model name without path prefix\n",
        "Chosen_model_EM = Chosen_model_EM.split(\"/\")[-1]"
      ],
      "metadata": {
        "id": "Y5rcw_XYMzep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments for model fine-tuning\n",
        "EM_Trn_Args = TrainingArguments(\n",
        "    output_dir=f\"{Chosen_model_EM}-finetuned-custom\", overwrite_output_dir=True,\n",
        "    learning_rate=2e-5, weight_decay=0.01,per_device_train_batch_size=Em_Btch_Size,\n",
        "    per_device_eval_batch_size=Em_Btch_Size, push_to_hub=False,\n",
        "    fp16=torch.cuda.is_available(),logging_steps=EM_Log_Steps,report_to=\"none\")"
      ],
      "metadata": {
        "id": "y7DxWryMNJCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load accuracy metric from HuggingFace evaluate library\n",
        "Acc_Met_EM = evaluate.load(\"accuracy\")\n",
        "\n",
        "# Define function to compute evaluation metrics\n",
        "def Cal_Metrics_EM(eval_pred):\n",
        "    \"\"\"\n",
        "    Computes accuracy metric for model evaluation.\n",
        "\n",
        "    Parameters:\n",
        "    eval_pred : A tuple containing logits of Raw model predictions and\n",
        "    labels of True labels.\n",
        "\n",
        "    Returns: Dictionary containing accuracy score.\n",
        "    \"\"\"\n",
        "    EM_Logits, EM_labels = eval_pred\n",
        "    EM_preds = torch.argmax(torch.tensor(EM_Logits), dim=-1)\n",
        "    return Acc_Met_EM.compute(predictions=EM_preds, references=EM_labels)"
      ],
      "metadata": {
        "id": "HyIqmNhKNetG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model using HuggingFace Trainer API\n",
        "EM_Trainer_det = Trainer(\n",
        "    model=Cust_Model_EM,\n",
        "    args=EM_Trn_Args,\n",
        "    train_dataset=Em_Train_DS,\n",
        "    eval_dataset=Em_Test_DS,\n",
        "    compute_metrics=Cal_Metrics_EM\n",
        ")\n",
        "# Start training\n",
        "EM_Trainer_det.train()"
      ],
      "metadata": {
        "id": "mJHX5NSmN5-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the trained model on the test set\n",
        "EM_Model_Eval = EM_Trainer_det.evaluate()\n",
        "# Print evaluation metrics of the model on Email Spam data\n",
        "print(\"Evaluation Results of the Model on Email Spam Data :\")\n",
        "for key, value in EM_Model_Eval.items():\n",
        "    print(f\"{key}: {value:.4f}\")"
      ],
      "metadata": {
        "id": "EPk79lHaOHp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions from the model on test data\n",
        "EM_Model_Pred = EM_Trainer_det.predict(Em_Test_DS)\n",
        "Org_Labels = EM_Model_Pred.label_ids\n",
        "EM_Pred_Labels = torch.argmax(torch.tensor(EM_Model_Pred.predictions), axis=1).numpy()\n",
        "# Compute and display confusion matrix\n",
        "Eval_EM_ConfMat = confusion_matrix(Org_Labels, EM_Pred_Labels)\n",
        "Disp_EM_Confmat = ConfusionMatrixDisplay(confusion_matrix=Eval_EM_ConfMat)\n",
        "Disp_EM_Confmat.plot(cmap=\"Blues\", values_format=\"d\")\n",
        "plt.title(\"Confusion Matrix for the Email Spam Detection Evaluation\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "S-BBKZdxObJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get raw text from the original test dataset\n",
        "EM_testing_Samples = Em_Test_DS[\"text\"]\n",
        "# Display 5 prediction samples with their true and predicted labels\n",
        "print(\"\\nPredicting the Lables with the Trained Models on Test Data Samples:\\n\")\n",
        "for i in range(5):\n",
        "    print(f\"Text: {EM_testing_Samples[i]}\")\n",
        "    print(f\"Predicted Label : {EM_Pred_Labels[i]}, True Label: {Org_Labels[i]}\")\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "id": "ws3ALMNjPIsD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}